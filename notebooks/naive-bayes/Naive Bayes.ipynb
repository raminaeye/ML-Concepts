{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc594ef",
   "metadata": {},
   "source": [
    "![alternative text](../../data/nb1_chatgpt.png)\n",
    "\n",
    "\n",
    "![alternative text](../../data/bayes.png)\n",
    "\n",
    "#### Posterior probability (updated probability after the evidence is considered) \n",
    "\n",
    "#### Prior probability (the probability before the evidence is considered) \n",
    "\n",
    "#### Likelihood (probability of the evidence, given the belief is true)\n",
    "\n",
    "#### Marginal probability (probability of the evidence, under any circumstance)\n",
    "\n",
    "![alternative text](../../data/nb2_chatgpt.png)\n",
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc331920",
   "metadata": {},
   "source": [
    "![alternative text](../../data/nb3_chatgpt.png)\n",
    "\n",
    "Feature Likelihoods (Conditional Probabilities): You estimate the likelihood of observing each feature (word or term) given each class. In the case of MNB, this involves calculating the conditional probabilities. These probabilities indicate how likely each feature is to appear in documents of each class. For MNB, you estimate the likelihood of observing each feature (word or term) given each class. This involves counting how often each feature appears in documents of each class and normalizing by the total count of features in that class. Laplace smoothing can be applied here. \n",
    "\n",
    "Once you've estimated the class priors and feature likelihoods for each class, the training process is complete, and you have a trained Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad7901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib.pylab import plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3bffe665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples 15076 and number of labels 20\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into features (text) and labels (newsgroup categories)\n",
    "X = newsgroups.data\n",
    "y = newsgroups.target\n",
    "names = newsgroups.target_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"number of training examples {len(X_train)} and number of labels {len(set(y_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50cd97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class CountVectorizer:\n",
    "    def __init__(self, max_features=None):\n",
    "        self.max_features = max_features\n",
    "        self.vocabulary_ = {}\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        # Tokenize and build the vocabulary\n",
    "        token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "        words = re.findall(token_pattern, \" \".join(documents).lower())\n",
    "        counts = Counter(words)\n",
    "        unique_words = set(words)\n",
    "        if self.max_features is not None:\n",
    "            unique_words = [x[0] for x in counts.most_common(self.max_features)]\n",
    "        self.vocabulary_ = {word: index for index, word in enumerate(unique_words)}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, documents):\n",
    "        # Transform documents into count vectors\n",
    "        if not self.vocabulary_:\n",
    "            raise ValueError(\"CountVectorizer has not been fitted.\")\n",
    "        \n",
    "        feature_names = list(self.vocabulary_.keys())\n",
    "        X = np.zeros((len(documents), len(feature_names)), dtype=int)\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            words = re.findall(r\"(?u)\\b\\w\\w+\\b\", doc.lower())\n",
    "            for word in words:\n",
    "                if word in self.vocabulary_:\n",
    "                    feature_index = self.vocabulary_[word]\n",
    "                    X[i, feature_index] += 1\n",
    "        \n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbffcb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 10000)\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the CountVectorizer\n",
    "num_vocab = 10000\n",
    "vectorizer = CountVectorizer(max_features=num_vocab)\n",
    "vectorizer.fit(X_train)\n",
    "# Transform the documents into count vectors\n",
    "X_train_tokens = vectorizer.transform(X_train)\n",
    "X_test_tokens = vectorizer.transform(X_test)\n",
    "print(X_train_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff75fed7",
   "metadata": {},
   "source": [
    "The parameter \"alpha\" represents Laplace smoothing (additive smoothing), which is used to avoid zero probabilities when a word has not been observed in a specific class. It prevents the algorithm from assigning zero likelihood to unseen words.\n",
    "\n",
    "Calculating the likelihoods for the features present in the document:\n",
    "![alternative text](../../data/ll_nb.png)\n",
    "![alternative text](../../data/ll_nb2.png)\n",
    "![alternative text](../../data/ll_nb3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5ddbfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNB:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "        self.class_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Calculate class priors\n",
    "        # P(class) represents the prior probability of a class.\n",
    "        # P(class) = (Number of samples in class) / (Total number of samples).\n",
    "        unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "        total_samples = len(y)\n",
    "        self.class_prior_ = class_counts / total_samples\n",
    "        self.classes_ = unique_classes\n",
    "\n",
    "        # Calculate conditional probabilities (log probabilities)\n",
    "        num_classes = len(unique_classes)\n",
    "        num_features = X.shape[1]\n",
    "        \n",
    "        # for every class\n",
    "        # P(word|class) represents the conditional probability of observing a word given a class.\n",
    "        # P(word|class) = (Count of word occurrences in documents of the class + alpha) / (Total count of words in documents of the class + alpha * Vocabulary size).\n",
    "        self.feature_log_prob_ = np.zeros((num_classes, num_features))\n",
    "        for i, cls in enumerate(unique_classes):\n",
    "            X_cls = X[y == cls] \n",
    "            total_word_count = X_cls.sum() + num_features * self.alpha\n",
    "            # For each word, how likely does it belong to a class?   \n",
    "            self.feature_log_prob_[i, :] = np.log((X_cls.sum(axis=0) + self.alpha) / total_word_count)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict class labels for input samples\n",
    "        return self.classes_[np.argmax(self.predict_log_proba(X), axis=1)]\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        # Calculate log probabilities of each class for input samples\n",
    "        # P(class|document) = (P(class) * P(document|class)) / P(document)\n",
    "        # log(P(class)) +  log(P(document|class)) = prior + likelihood\n",
    "        # P(document) is constant\n",
    "        # likelihood : P(document|class) = P(feature_1|class) * P(feature_2|class) * ... * P(feature_n|class)\n",
    "        # X * self.feature_log_prob_ : likelihood of observing a specific set of features for a given class \n",
    "        prior = np.log(self.class_prior_)\n",
    "        likelihood = np.dot(X, self.feature_log_prob_.T)\n",
    "        posterior_probs = prior + likelihood\n",
    "        \n",
    "        return posterior_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ccfb7927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model size (20, 10000) \n",
      " and class prior [0.04298222 0.05114089 0.05240117 0.05299814 0.05027859 0.05127355\n",
      " 0.05187052 0.05266649 0.05492173 0.05193685 0.0531308  0.05240117\n",
      " 0.05187052 0.05279915 0.05293181 0.05273282 0.04789069 0.05027859\n",
      " 0.04085964 0.03263465]\n"
     ]
    }
   ],
   "source": [
    "MNB = MultinomialNB(0.1)\n",
    "MNB.fit(X_train_tokens,y_train)\n",
    "\n",
    "print(f\" model size {MNB.feature_log_prob_.shape} \\n and class prior {MNB.class_prior_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "073ad5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10000)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accuracy = np.sum(MNB.predict(X_test_tokens) == y_test)/len(y_test)\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0982af",
   "metadata": {},
   "source": [
    "# Let's walk through an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62295080",
   "metadata": {},
   "source": [
    "![alternative text](../../data/ll_nb4.png)\n",
    "![alternative text](../../data/ll_nb5.png)\n",
    "![alternative text](../../data/ll_nb6.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2ee11dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I need to port several OS/2 PM applications to X (OpenWindows or Motif),\n",
      "and desperately need any information on how to go about doing this (short\n",
      "of a complete rewrite.\n",
      " \n",
      "Are there any tool to make porting easer?\n",
      "Any References?\n",
      "Any talent out there to hire to do this?\n",
      "I will even take an OS/2 Presentation Mgr emulator for sun!\n",
      " \n",
      "Any, and all replies (except flames) welcome!\n",
      " \n",
      " \n",
      "Brian Colaric \n",
      "\n",
      " label :  comp.windows.x\n"
     ]
    }
   ],
   "source": [
    "index = 20\n",
    "print(X_test[index],'\\n\\n label : ',names[y_test[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0be856ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tokens (10000,) model size  (20, 10000)\n"
     ]
    }
   ],
   "source": [
    "test_tokens = X_test_tokens[index]\n",
    "model = MNB.feature_log_prob_ # log likelihood of each word give each class\n",
    "print(\"test_tokens\", np.shape(test_tokens), \"model size \",model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1fc5ebda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comp.windows.x'"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# likehood of test sample given each class * prior = likehood of the class given sample \n",
    "posterior = model @ test_tokens + MNB.class_prior_ \n",
    "names[np.argmax(posterior)] # prediction "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
