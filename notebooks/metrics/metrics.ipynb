{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e80dc022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate confusion matrix from true and predicted labels\n",
    "def calculate_confusion_matrix(true_labels, predicted_labels, class_labels):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for multiclass classification.\n",
    "    Args:\n",
    "        true_labels (list): True class labels.\n",
    "        predicted_labels (list): Predicted class labels.\n",
    "        class_labels (list): List of all possible class labels.\n",
    "    Returns:\n",
    "        np.ndarray: Confusion matrix with shape (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    num_classes = len(class_labels)\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    class_to_index = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        conf_matrix[class_to_index[true], class_to_index[pred]] += 1\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "# Function to calculate precision, recall, and F1-score for each class\n",
    "def calculate_class_metrics(conf_matrix):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, and F1-score for each class from the confusion matrix.\n",
    "    Args:\n",
    "        conf_matrix (np.ndarray): Confusion matrix.\n",
    "    Returns:\n",
    "        dict: Dictionary with metrics for each class.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for i in range(len(conf_matrix)):\n",
    "        true_positive = conf_matrix[i, i]\n",
    "        predicted_positive = conf_matrix[:, i].sum()\n",
    "        actual_positive = conf_matrix[i, :].sum()\n",
    "\n",
    "        precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "        recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics[i] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"support\": actual_positive,\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to calculate overall accuracy\n",
    "def calculate_accuracy(conf_matrix):\n",
    "    \"\"\"\n",
    "    Computes the overall accuracy from the confusion matrix.\n",
    "    Args:\n",
    "        conf_matrix (np.ndarray): Confusion matrix.\n",
    "    Returns:\n",
    "        float: Overall accuracy.\n",
    "    \"\"\"\n",
    "    correct_predictions = np.trace(conf_matrix)\n",
    "    total_predictions = conf_matrix.sum()\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# Function to calculate macro and weighted averages\n",
    "def calculate_macro_weighted_averages(metrics, total_samples):\n",
    "    \"\"\"\n",
    "    Computes macro and weighted averages for precision, recall, and F1-score.\n",
    "    Args:\n",
    "        metrics (dict): Metrics for each class.\n",
    "        total_samples (int): Total number of samples.\n",
    "    Returns:\n",
    "        dict: Dictionary with macro and weighted averages.\n",
    "    \"\"\"\n",
    "    macro_precision = np.mean([m[\"precision\"] for m in metrics.values()])\n",
    "    macro_recall = np.mean([m[\"recall\"] for m in metrics.values()])\n",
    "    macro_f1 = np.mean([m[\"f1_score\"] for m in metrics.values()])\n",
    "\n",
    "    weighted_precision = sum(m[\"precision\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "    weighted_recall = sum(m[\"recall\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "    weighted_f1 = sum(m[\"f1_score\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "\n",
    "    return {\n",
    "        \"macro\": {\"precision\": macro_precision, \"recall\": macro_recall, \"f1_score\": macro_f1},\n",
    "        \"weighted\": {\"precision\": weighted_precision, \"recall\": weighted_recall, \"f1_score\": weighted_f1},\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# Simulate some true labels and predicted labels for a 3-class classification task\n",
    "true_labels = np.random.choice(['Class A', 'Class B', 'Class C'], size=100, p=[0.4, 0.35, 0.25])\n",
    "predicted_labels = np.random.choice(['Class A', 'Class B', 'Class C'], size=100, p=[0.4, 0.35, 0.25])\n",
    "\n",
    "\n",
    "class_labels = ['Class A', 'Class B', 'Class C']\n",
    "conf_matrix_custom = calculate_confusion_matrix(true_labels, predicted_labels, class_labels)\n",
    "metrics_custom = calculate_class_metrics(conf_matrix_custom)\n",
    "accuracy_custom = calculate_accuracy(conf_matrix_custom)\n",
    "averages_custom = calculate_macro_weighted_averages(metrics_custom, len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a0a23f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro</th>\n",
       "      <th>weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.325116</td>\n",
       "      <td>0.344628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.337703</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.324712</td>\n",
       "      <td>0.332370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              macro  weighted\n",
       "precision  0.325116  0.344628\n",
       "recall     0.337703  0.330000\n",
       "f1_score   0.324712  0.332370"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(averages_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7bd27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0dd7daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1          2\n",
       "precision   0.363636   0.378378   0.233333\n",
       "recall      0.285714   0.358974   0.368421\n",
       "f1_score    0.320000   0.368421   0.285714\n",
       "support    42.000000  39.000000  19.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fe743c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 17, 13],\n",
       "       [15, 14, 10],\n",
       "       [ 6,  6,  7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d73a7903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                Predicted Class A  Predicted Class B  Predicted Class C\n",
       " Actual Class A                 12                 17                 13\n",
       " Actual Class B                 15                 14                 10\n",
       " Actual Class C                  6                  6                  7,\n",
       "               precision    recall  f1-score  support\n",
       " Class A        0.363636  0.285714  0.320000    42.00\n",
       " Class B        0.378378  0.358974  0.368421    39.00\n",
       " Class C        0.233333  0.368421  0.285714    19.00\n",
       " accuracy       0.330000  0.330000  0.330000     0.33\n",
       " macro avg      0.325116  0.337703  0.324712   100.00\n",
       " weighted avg   0.344628  0.330000  0.332370   100.00,\n",
       " 0.33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd \n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=['Class A', 'Class B', 'Class C'])\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, predicted_labels, target_names=['Class A', 'Class B', 'Class C'], output_dict=True)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Convert confusion matrix and report to DataFrame for better readability\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=['Actual Class A', 'Actual Class B', 'Actual Class C'], \n",
    "                              columns=['Predicted Class A', 'Predicted Class B', 'Predicted Class C'])\n",
    "\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "conf_matrix_df, class_report_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c909e643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18181818181818182, 0.6, 1.2565790685485896)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate Character Error Rate (CER)\n",
    "def calculate_character_error_rate(true_strings, predicted_strings):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate (CER).\n",
    "    Args:\n",
    "        true_strings (list of str): List of ground truth strings.\n",
    "        predicted_strings (list of str): List of predicted strings.\n",
    "    Returns:\n",
    "        float: Character Error Rate (CER).\n",
    "    \"\"\"\n",
    "    total_characters = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    for true, pred in zip(true_strings, predicted_strings):\n",
    "        total_characters += len(true)\n",
    "        total_errors += levenshtein_distance(true, pred)\n",
    "\n",
    "    return total_errors / total_characters if total_characters > 0 else 0\n",
    "\n",
    "# Function to calculate Word Error Rate (WER)\n",
    "def calculate_word_error_rate(true_strings, predicted_strings):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate (WER).\n",
    "    Args:\n",
    "        true_strings (list of str): List of ground truth strings.\n",
    "        predicted_strings (list of str): List of predicted strings.\n",
    "    Returns:\n",
    "        float: Word Error Rate (WER).\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    for true, pred in zip(true_strings, predicted_strings):\n",
    "        true_words = true.split()\n",
    "        pred_words = pred.split()\n",
    "        total_words += len(true_words)\n",
    "        total_errors += levenshtein_distance(true_words, pred_words)\n",
    "\n",
    "    return total_errors / total_words if total_words > 0 else 0\n",
    "\n",
    "# Helper function to calculate Levenshtein distance\n",
    "def levenshtein_distance(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Computes the Levenshtein distance between two sequences.\n",
    "    Args:\n",
    "        seq1 (str or list): First sequence.\n",
    "        seq2 (str or list): Second sequence.\n",
    "    Returns:\n",
    "        int: Levenshtein distance.\n",
    "    \"\"\"\n",
    "    len_seq1, len_seq2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len_seq1 + 1, len_seq2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len_seq1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_seq2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_seq1 + 1):\n",
    "        for j in range(1, len_seq2 + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "\n",
    "    return dp[len_seq1][len_seq2]\n",
    "\n",
    "# Function to calculate Perplexity\n",
    "def calculate_perplexity(probabilities):\n",
    "    \"\"\"\n",
    "    Computes the Perplexity score.\n",
    "    Args:\n",
    "        probabilities (list of float): List of predicted probabilities for the true labels.\n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    n = len(probabilities)\n",
    "    log_sum = sum(np.log(p) for p in probabilities if p > 0)  # Avoid log(0)\n",
    "    return np.exp(-log_sum / n) if n > 0 else float('inf')\n",
    "\n",
    "# Example usage for CER, WER, and Perplexity\n",
    "true_strings = [\"hello world\", \"machine learning\", \"openai\"]\n",
    "predicted_strings = [\"helo world\", \"machine learn\", \"openia\"]\n",
    "probabilities = [0.8, 0.7, 0.9]  # Example probabilities of the true labels\n",
    "\n",
    "cer = calculate_character_error_rate(true_strings, predicted_strings)\n",
    "wer = calculate_word_error_rate(true_strings, predicted_strings)\n",
    "perplexity = calculate_perplexity(probabilities)\n",
    "\n",
    "cer, wer, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0014f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
