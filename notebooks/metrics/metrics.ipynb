{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c419ac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate confusion matrix from true and predicted labels\n",
    "def calculate_confusion_matrix(true_labels, predicted_labels, class_labels):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix for multiclass classification.\n",
    "    Args:\n",
    "        true_labels (list): True class labels.\n",
    "        predicted_labels (list): Predicted class labels.\n",
    "        class_labels (list): List of all possible class labels.\n",
    "    Returns:\n",
    "        np.ndarray: Confusion matrix with shape (num_classes, num_classes).\n",
    "    \"\"\"\n",
    "    num_classes = len(class_labels)\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "    class_to_index = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "    for true, pred in zip(true_labels, predicted_labels):\n",
    "        conf_matrix[class_to_index[true], class_to_index[pred]] += 1\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "# Function to calculate precision, recall, and F1-score for each class\n",
    "def calculate_class_metrics(conf_matrix):\n",
    "    \"\"\"\n",
    "    Computes precision, recall, and F1-score for each class from the confusion matrix.\n",
    "    Args:\n",
    "        conf_matrix (np.ndarray): Confusion matrix.\n",
    "    Returns:\n",
    "        dict: Dictionary with metrics for each class.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    for i in range(len(conf_matrix)):\n",
    "        true_positive = conf_matrix[i, i]\n",
    "        predicted_positive = conf_matrix[:, i].sum()\n",
    "        actual_positive = conf_matrix[i, :].sum()\n",
    "\n",
    "        precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "        recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        metrics[i] = {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"support\": actual_positive,\n",
    "        }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Function to calculate overall accuracy\n",
    "def calculate_accuracy(conf_matrix):\n",
    "    \"\"\"\n",
    "    Computes the overall accuracy from the confusion matrix.\n",
    "    Args:\n",
    "        conf_matrix (np.ndarray): Confusion matrix.\n",
    "    Returns:\n",
    "        float: Overall accuracy.\n",
    "    \"\"\"\n",
    "    correct_predictions = np.trace(conf_matrix)\n",
    "    total_predictions = conf_matrix.sum()\n",
    "    return correct_predictions / total_predictions\n",
    "\n",
    "# Function to calculate macro and weighted averages\n",
    "def calculate_macro_weighted_averages(metrics, total_samples):\n",
    "    \"\"\"\n",
    "    Computes macro and weighted averages for precision, recall, and F1-score.\n",
    "    Args:\n",
    "        metrics (dict): Metrics for each class.\n",
    "        total_samples (int): Total number of samples.\n",
    "    Returns:\n",
    "        dict: Dictionary with macro and weighted averages.\n",
    "    \"\"\"\n",
    "    macro_precision = np.mean([m[\"precision\"] for m in metrics.values()])\n",
    "    macro_recall = np.mean([m[\"recall\"] for m in metrics.values()])\n",
    "    macro_f1 = np.mean([m[\"f1_score\"] for m in metrics.values()])\n",
    "\n",
    "    weighted_precision = sum(m[\"precision\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "    weighted_recall = sum(m[\"recall\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "    weighted_f1 = sum(m[\"f1_score\"] * m[\"support\"] for m in metrics.values()) / total_samples\n",
    "\n",
    "    return {\n",
    "        \"macro\": {\"precision\": macro_precision, \"recall\": macro_recall, \"f1_score\": macro_f1},\n",
    "        \"weighted\": {\"precision\": weighted_precision, \"recall\": weighted_recall, \"f1_score\": weighted_f1},\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# Simulate some true labels and predicted labels for a 3-class classification task\n",
    "true_labels = np.random.choice(['Class A', 'Class B', 'Class C'], size=100, p=[0.4, 0.35, 0.25])\n",
    "predicted_labels = np.random.choice(['Class A', 'Class B', 'Class C'], size=100, p=[0.4, 0.35, 0.25])\n",
    "\n",
    "\n",
    "class_labels = ['Class A', 'Class B', 'Class C']\n",
    "conf_matrix_custom = calculate_confusion_matrix(true_labels, predicted_labels, class_labels)\n",
    "metrics_custom = calculate_class_metrics(conf_matrix_custom)\n",
    "accuracy_custom = calculate_accuracy(conf_matrix_custom)\n",
    "averages_custom = calculate_macro_weighted_averages(metrics_custom, len(true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ac6e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>macro</th>\n",
       "      <th>weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.325116</td>\n",
       "      <td>0.344628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.337703</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.324712</td>\n",
       "      <td>0.332370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              macro  weighted\n",
       "precision  0.325116  0.344628\n",
       "recall     0.337703  0.330000\n",
       "f1_score   0.324712  0.332370"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(averages_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2330827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30da790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_score</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>42.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0          1          2\n",
       "precision   0.363636   0.378378   0.233333\n",
       "recall      0.285714   0.358974   0.368421\n",
       "f1_score    0.320000   0.368421   0.285714\n",
       "support    42.000000  39.000000  19.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(metrics_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32a9687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 17, 13],\n",
       "       [15, 14, 10],\n",
       "       [ 6,  6,  7]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matrix_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a935f7f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                Predicted Class A  Predicted Class B  Predicted Class C\n",
       " Actual Class A                 12                 17                 13\n",
       " Actual Class B                 15                 14                 10\n",
       " Actual Class C                  6                  6                  7,\n",
       "               precision    recall  f1-score  support\n",
       " Class A        0.363636  0.285714  0.320000    42.00\n",
       " Class B        0.378378  0.358974  0.368421    39.00\n",
       " Class C        0.233333  0.368421  0.285714    19.00\n",
       " accuracy       0.330000  0.330000  0.330000     0.33\n",
       " macro avg      0.325116  0.337703  0.324712   100.00\n",
       " weighted avg   0.344628  0.330000  0.332370   100.00,\n",
       " 0.33)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd \n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=['Class A', 'Class B', 'Class C'])\n",
    "\n",
    "# Generate classification report\n",
    "class_report = classification_report(true_labels, predicted_labels, target_names=['Class A', 'Class B', 'Class C'], output_dict=True)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Convert confusion matrix and report to DataFrame for better readability\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix, \n",
    "                              index=['Actual Class A', 'Actual Class B', 'Actual Class C'], \n",
    "                              columns=['Predicted Class A', 'Predicted Class B', 'Predicted Class C'])\n",
    "\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "conf_matrix_df, class_report_df, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec74d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18181818181818182, 0.6, 1.2565790685485896)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate Character Error Rate (CER)\n",
    "def calculate_character_error_rate(true_strings, predicted_strings):\n",
    "    \"\"\"\n",
    "    Computes the Character Error Rate (CER).\n",
    "    Args:\n",
    "        true_strings (list of str): List of ground truth strings.\n",
    "        predicted_strings (list of str): List of predicted strings.\n",
    "    Returns:\n",
    "        float: Character Error Rate (CER).\n",
    "    \"\"\"\n",
    "    total_characters = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    for true, pred in zip(true_strings, predicted_strings):\n",
    "        total_characters += len(true)\n",
    "        total_errors += levenshtein_distance(true, pred)\n",
    "\n",
    "    return total_errors / total_characters if total_characters > 0 else 0\n",
    "\n",
    "# Function to calculate Word Error Rate (WER)\n",
    "def calculate_word_error_rate(true_strings, predicted_strings):\n",
    "    \"\"\"\n",
    "    Computes the Word Error Rate (WER).\n",
    "    Args:\n",
    "        true_strings (list of str): List of ground truth strings.\n",
    "        predicted_strings (list of str): List of predicted strings.\n",
    "    Returns:\n",
    "        float: Word Error Rate (WER).\n",
    "    \"\"\"\n",
    "    total_words = 0\n",
    "    total_errors = 0\n",
    "\n",
    "    for true, pred in zip(true_strings, predicted_strings):\n",
    "        true_words = true.split()\n",
    "        pred_words = pred.split()\n",
    "        total_words += len(true_words)\n",
    "        total_errors += levenshtein_distance(true_words, pred_words)\n",
    "\n",
    "    return total_errors / total_words if total_words > 0 else 0\n",
    "\n",
    "# Helper function to calculate Levenshtein distance\n",
    "def levenshtein_distance(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Computes the Levenshtein distance between two sequences.\n",
    "    Args:\n",
    "        seq1 (str or list): First sequence.\n",
    "        seq2 (str or list): Second sequence.\n",
    "    Returns:\n",
    "        int: Levenshtein distance.\n",
    "    \"\"\"\n",
    "    len_seq1, len_seq2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len_seq1 + 1, len_seq2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len_seq1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_seq2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_seq1 + 1):\n",
    "        for j in range(1, len_seq2 + 1):\n",
    "            if seq1[i - 1] == seq2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1])\n",
    "\n",
    "    return dp[len_seq1][len_seq2]\n",
    "\n",
    "# Function to calculate Perplexity\n",
    "def calculate_perplexity(probabilities):\n",
    "    \"\"\"\n",
    "    Computes the Perplexity score.\n",
    "    Args:\n",
    "        probabilities (list of float): List of predicted probabilities for the true labels.\n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    n = len(probabilities)\n",
    "    log_sum = sum(np.log(p) for p in probabilities if p > 0)  # Avoid log(0)\n",
    "    return np.exp(-log_sum / n) if n > 0 else float('inf')\n",
    "\n",
    "# Example usage for CER, WER, and Perplexity\n",
    "true_strings = [\"hello world\", \"machine learning\", \"openai\"]\n",
    "predicted_strings = [\"helo world\", \"machine learn\", \"openia\"]\n",
    "probabilities = [0.8, 0.7, 0.9]  # Example probabilities of the true labels\n",
    "\n",
    "cer = calculate_character_error_rate(true_strings, predicted_strings)\n",
    "wer = calculate_word_error_rate(true_strings, predicted_strings)\n",
    "perplexity = calculate_perplexity(probabilities)\n",
    "\n",
    "cer, wer, perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5412244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8483073030032446, 2.3356898886410007)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate cross-entropy loss\n",
    "def cross_entropy_loss(true_labels, predicted_probs):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss for a batch of predictions.\n",
    "    Args:\n",
    "        true_labels (np.ndarray): One-hot encoded true labels (N x C).\n",
    "        predicted_probs (np.ndarray): Predicted probabilities (N x C).\n",
    "    Returns:\n",
    "        float: Average cross-entropy loss.\n",
    "    \"\"\"\n",
    "    # Add a small epsilon to avoid log(0)\n",
    "    epsilon = 1e-12\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1.0 - epsilon)\n",
    "    \n",
    "    # Compute the cross-entropy loss for each sample\n",
    "    losses = -np.sum(true_labels * np.log(predicted_probs), axis=1)\n",
    "    \n",
    "    # Return the average loss\n",
    "    return np.mean(losses)\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def perplexity(true_labels, predicted_probs):\n",
    "    \"\"\"\n",
    "    Computes perplexity from cross-entropy loss.\n",
    "    Args:\n",
    "        true_labels (np.ndarray): One-hot encoded true labels (N x C).\n",
    "        predicted_probs (np.ndarray): Predicted probabilities (N x C).\n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    ce_loss = cross_entropy_loss(true_labels, predicted_probs)\n",
    "    return np.exp(ce_loss)\n",
    "\n",
    "# Example usage\n",
    "true_labels = np.array([\n",
    "    [1, 0, 0],  # True label for the first sample\n",
    "    [0, 1, 0],  # True label for the second sample\n",
    "    [0, 0, 1],\n",
    "    [1, 0, 0], # True label for the third sample\n",
    "])\n",
    "\n",
    "predicted_probs = np.array([\n",
    "    [0.7, 0.2, 0.1],  # Predicted probabilities for the first sample\n",
    "    [0.1, 0.8, 0.1],  # Predicted probabilities for the second sample\n",
    "    [0.2, 0.2, 0.6],  # Predicted probabilities for the third sample\n",
    "    [0.1, 0.1, 0.8],\n",
    "])\n",
    "\n",
    "ce_loss = cross_entropy_loss(true_labels, predicted_probs)\n",
    "pp = perplexity(true_labels, predicted_probs)\n",
    "\n",
    "ce_loss, pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27f79921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9746318461970762"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to compute cosine similarity between two vectors\n",
    "def cosine_similarity(A, B):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "    Args:\n",
    "        A (np.ndarray): First vector.\n",
    "        B (np.ndarray): Second vector.\n",
    "    Returns:\n",
    "        float: Cosine similarity between A and B.\n",
    "    \"\"\"\n",
    "    # Compute dot product between A and B\n",
    "    dot_product = np.dot(A, B)\n",
    "    \n",
    "    # Compute the magnitudes of A and B\n",
    "    magnitude_A = np.sqrt(np.sum(A**2))\n",
    "    magnitude_B = np.sqrt(np.sum(B**2))\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "# Example usage\n",
    "vector_A = np.array([1, 2, 3])\n",
    "vector_B = np.array([4, 5, 6])\n",
    "\n",
    "similarity = cosine_similarity(vector_A, vector_B)\n",
    "similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "353fffe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7598356856525962"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def n_grams(text, n):\n",
    "    \"\"\"\n",
    "    Generate n-grams from a list of words.\n",
    "    Args:\n",
    "        text (list): List of words (tokens).\n",
    "        n (int): The n-gram length.\n",
    "    Returns:\n",
    "        list: A list of n-grams (tuples of n words).\n",
    "    \"\"\"\n",
    "    return [tuple(text[i:i + n]) for i in range(len(text) - n + 1)]\n",
    "\n",
    "def precision(candidate, references, n):\n",
    "    \"\"\"\n",
    "    Calculate n-gram precision.\n",
    "    Args:\n",
    "        candidate (list): Candidate (generated) text.\n",
    "        references (list of lists): List of reference texts (ground truth).\n",
    "        n (int): The n-gram length.\n",
    "    Returns:\n",
    "        float: Precision for n-grams.\n",
    "    \"\"\"\n",
    "    candidate_ngrams = n_grams(candidate, n)\n",
    "    reference_ngrams = []\n",
    "    for ref in references:\n",
    "        reference_ngrams.extend(n_grams(ref, n))\n",
    "    \n",
    "    candidate_ngrams_count = Counter(candidate_ngrams)\n",
    "    reference_ngrams_count = Counter(reference_ngrams)\n",
    "    \n",
    "    # Count matching n-grams between candidate and references\n",
    "    match_count = 0\n",
    "    for ng in candidate_ngrams_count:\n",
    "        match_count += min(candidate_ngrams_count[ng], reference_ngrams_count.get(ng, 0))\n",
    "    \n",
    "    return match_count / len(candidate_ngrams) if len(candidate_ngrams) > 0 else 0\n",
    "\n",
    "def brevity_penalty(candidate, references):\n",
    "    \"\"\"\n",
    "    Calculate the brevity penalty.\n",
    "    Args:\n",
    "        candidate (list): Candidate (generated) text.\n",
    "        references (list of lists): List of reference texts (ground truth).\n",
    "    Returns:\n",
    "        float: Brevity penalty.\n",
    "    \"\"\"\n",
    "    candidate_length = len(candidate)\n",
    "    reference_lengths = [len(ref) for ref in references]\n",
    "    closest_ref_length = min(reference_lengths, key=lambda x: (abs(x - candidate_length), x))\n",
    "    \n",
    "    if candidate_length > closest_ref_length:\n",
    "        return 1\n",
    "    else:\n",
    "        return np.exp(1 - closest_ref_length / candidate_length) if candidate_length > 0 else 0\n",
    "\n",
    "def bleu_score(candidate, references, max_n=4):\n",
    "    \"\"\"\n",
    "    Compute BLEU score for a candidate translation against reference translations.\n",
    "    Args:\n",
    "        candidate (list): Candidate (generated) text (tokenized).\n",
    "        references (list of lists): List of reference texts (ground truth) (tokenized).\n",
    "        max_n (int): The maximum n-gram length to consider.\n",
    "    Returns:\n",
    "        float: BLEU score.\n",
    "    \"\"\"\n",
    "    p_n = []\n",
    "    \n",
    "    # Calculate precision for n-grams from 1 to max_n\n",
    "    for n in range(1, max_n + 1):\n",
    "        p_n.append(precision(candidate, references, n) + 1e-12)\n",
    "    \n",
    "    # Calculate geometric mean of precisions\n",
    "    p_n_product = np.sum(np.log(p_n ))\n",
    "    geometric_mean = np.exp(p_n_product * (1/max_n))\n",
    "    \n",
    "    # Apply brevity penalty\n",
    "    bp = brevity_penalty(candidate, references)\n",
    "    \n",
    "    return bp * geometric_mean\n",
    "\n",
    "# Example usage:\n",
    "candidate = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mats\"]\n",
    "references = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"a\", \"cat\", \"is\", \"sitting\", \"on\", \"the\", \"mat\"]\n",
    "]\n",
    "\n",
    "bleu = bleu_score(candidate, references)\n",
    "bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8082db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision(candidate, references, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de209d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'cat', 'sat', 'on'): 1,\n",
       "         ('cat', 'sat', 'on', 'the'): 1,\n",
       "         ('sat', 'on', 'the', 'mat'): 1})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_ngrams_count = Counter(candidate_ngrams)\n",
    "candidate_ngrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2bca810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'recall': 0.8333333333333334, 'precision': 0.8333333333333334, 'f1': 0.8333333333333334}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def lcs_length(candidate, reference):\n",
    "    \"\"\"\n",
    "    Compute the length of the Longest Common Subsequence (LCS) between two sequences.\n",
    "    Args:\n",
    "        candidate (list): Tokenized candidate text.\n",
    "        reference (list): Tokenized reference text.\n",
    "    Returns:\n",
    "        int: Length of the LCS.\n",
    "    \"\"\"\n",
    "    m, n = len(candidate), len(reference)\n",
    "    dp = np.zeros((m + 1, n + 1), dtype=int)\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if candidate[i - 1] == reference[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l(candidate, references):\n",
    "    \"\"\"\n",
    "    Compute the ROUGE-L score for a candidate text against reference texts.\n",
    "    Args:\n",
    "        candidate (list): Tokenized candidate text.\n",
    "        references (list of lists): List of tokenized reference texts.\n",
    "    Returns:\n",
    "        dict: ROUGE-L scores (recall, precision, F1).\n",
    "    \"\"\"\n",
    "    best_lcs = 0\n",
    "    best_reference = None\n",
    "\n",
    "    # Compute LCS for each reference and choose the best match\n",
    "    for reference in references:\n",
    "        lcs = lcs_length(candidate, reference)\n",
    "        if lcs > best_lcs:\n",
    "            best_lcs = lcs\n",
    "            best_reference = reference\n",
    "\n",
    "    if best_reference is None:  # No valid reference\n",
    "        return {\"recall\": 0.0, \"precision\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    # Calculate precision, recall, and F1\n",
    "    lcs = best_lcs\n",
    "    recall = lcs / len(best_reference) if best_reference else 0.0\n",
    "    precision = lcs / len(candidate) if candidate else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    \n",
    "    return {\"recall\": recall, \"precision\": precision, \"f1\": f1}\n",
    "\n",
    "# Example usage\n",
    "candidate = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "references = [\n",
    "    [\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"],\n",
    "    [\"a\", \"cat\", \"st\", \"on\", \"the\", \"mat\"]\n",
    "]\n",
    "\n",
    "rouge_scores = rouge_l(candidate, references)\n",
    "print(rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c412a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005946035575023813"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu = bleu_score(candidate, references)\n",
    "bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d865d788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
