{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f720bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import defaultdict, Counter\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b41a16",
   "metadata": {},
   "source": [
    "# Building your own Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06bf8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "train_iter = AG_NEWS(split=\"train\")\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "\n",
    "ag_news_label = {1: \"World\", 2: \"Sports\", 3: \"Business\", 4: \"Sci/Tec\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1346dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "num_samples = -1\n",
    "# normalize and tokenize \n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        text = text.replace(\"\\\\\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\"-\",\" \").lower()\n",
    "        yield tokenizer(text)\n",
    "        \n",
    "\n",
    "token_iter = yield_tokens(train_iter)\n",
    "token_count = defaultdict(int)\n",
    "while True: \n",
    "    try:\n",
    "        tokens = next(token_iter)\n",
    "        num_samples += 1\n",
    "        for x in tokens:\n",
    "            token_count[x] += 1         \n",
    "    except StopIteration as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3b7f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples 119999\n"
     ]
    }
   ],
   "source": [
    "print(\"number of samples %d\" %num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80c05b",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 20 # min number of counts for a word to be a token\n",
    "token_map = {}\n",
    "tid = 0\n",
    "token_map['<unk>'] = tid\n",
    "for k,v in token_count.items():\n",
    "    if v >= threshold : \n",
    "        tid += 1\n",
    "        token_map[k] = tid\n",
    "        \n",
    "text_pipeline  = lambda x: [token_map[y] if y in token_map else 0 for y in tokenizer(x)  ]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "vocab_size = len(token_map)+1\n",
    "\n",
    "text = 'my tokenizer is nice'\n",
    "text_pipeline(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch): # collate_fn will take care of padding\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        \n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) \n",
    "        \n",
    "        label_list.append(label_pipeline(_label))# adding 1 to label \n",
    "        text_list.append(processed_text) # tokenized texts\n",
    "        offsets.append(processed_text.size(0)) # text size for padding \n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets    = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list  = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "692eab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53dca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(\n",
    "                \"| epoch {:3d} |\"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, idx, total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f0277f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emsize = 64\n",
    "EPOCHS = 2  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7346f326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text.lower()))\n",
    "        print(text)\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9efcf343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  42,   50,  449,    0, 1264,   68,    7, 1568,  455, 1325,   42, 1308])\n",
      "This is a Business news\n"
     ]
    }
   ],
   "source": [
    "ex_text_str = \"in another news aliens took over the planet while floating in space \"\n",
    "\n",
    "print(\"This is a %s news\" % ag_news_label[predict(ex_text_str, text_pipeline)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88157ba6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m1.0\u001b[39m]),requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Step 3\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 4 \u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "# Step 1\n",
    "y = 4 * x + 3\n",
    "# Step 2\n",
    "x = torch.autograd.Variable(torch.Tensor([1.0]),requires_grad=True)\n",
    "# Step 3\n",
    "y.backward()\n",
    "# Step 4 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Model definition\n",
    "class Net(nn.Module):\n",
    "    # Define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer = Linear(n_inputs, 1)\n",
    "        self.activation = Sigmoid()\n",
    "\n",
    "    # Step 2: Forward propagate input\n",
    "    def forward(self, X):\n",
    "        X = self.layer(X)\n",
    "        X = self.activation(X)\n",
    "        return X\n",
    "\n",
    "# Step 3: Test model\n",
    "my_nn = Net()\n",
    "result = my_nn(some_data)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84691f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.\n",
    "\n",
    "for i, data in enumerate(training_batch_set):\n",
    "\n",
    "    # 1. Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "\n",
    "    # 2. Zero your gradients for every batch!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 3. Make predictions for this batch\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # 4. Compute the loss and its gradients\n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # 6. Gather data and for report\n",
    "    running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be89e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and 'fc1' in name:\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222da563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Step 2\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f164f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define your custom loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        # compute your custom loss function\n",
    "        loss = ... # compute the loss\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a69193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your model and loss function\n",
    "model = ...\n",
    "optimizer = ...\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Rest of the code to evaluate the model goes here...\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4b6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a0426e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7917e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classify(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_size = 2 \n",
    "        output_size = 3\n",
    "        hidden_size = 16\n",
    "        self.mid_layers = []\n",
    "        for _ in range(5):\n",
    "            self.mid_layers.append(nn.Linear(hidden_size,hidden_size))\n",
    "            \n",
    "        self.mid_layers    = nn.Sequential(*self.mid_layers)\n",
    "        self.input_layers  = nn.Sequential(nn.Linear(input_size,hidden_size)\n",
    "                                    ,nn.ReLU())\n",
    "        self.output_layers = nn.Linear(hidden_size,output_size)\n",
    "                                    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = self.input_layers(x)\n",
    "        z = self.mid_layers(z)\n",
    "        z = self.output_layers(z)\n",
    "        return z \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ef183",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = Classify()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd013062",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batches =  [torch.ones((100,2),device=device) for _ in range(5)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8bf242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "from torch.distributions.kl import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2487586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Categorical(logits=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9fb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for input_data in input_batches:\n",
    "    output = model(input_data)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    target = torch.randn(100,3)\n",
    "    loss = criterion(output,target)\n",
    "    losses.append(loss.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ef07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e737226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a927bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
